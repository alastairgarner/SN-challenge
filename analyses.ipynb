{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Churn analysis\n",
    "\n",
    "Here we will try and identify variables that correlate with whether a user churns or not. \n",
    "\n",
    "To do this, we'll first load and transform the data into a format that will make running the correlations easy. Given a mix of continuous and discrete variables, we'll need to separately run pearson and chi2 correlations to detemine the significance of the variables for churn. Then we'll plot the data, to allow visual confirm any correlation we might have identified. Finally, we'll built a model to try and predict whether users will churn or not, using the scikit-learn package.\n",
    "\n",
    "**Data**\n",
    "\n",
    "USERS.json: A list of users containing some descriptive data (age/height/weight), as well as their answers to the sign-up quiz (motivation/challenge level).\n",
    "\n",
    "EVENTS.json: A list of all the events logged by users (articles read, weigh-ins, etc)\n",
    "\n",
    "MESSAGES.json: A log of all messages sent by users. This does not include message content, but rather a sentiment score and other features to determine whether questions were asked, or peripherals were mentioned. \n",
    "\n",
    "\n",
    "**Notes**\n",
    "\n",
    "Where plots or csv files are generated in this script, I've commented out the lines that actually write the files. This is to avoid accident overwriting of the figures, if you choose to edit the script yourself. I've included the figures in the repo (in the 'figs' folder), so you browse them there. Else, if you want to uncomment the lines and save the plots yourself, feel free to!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First we'll load our dependencies."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dependencies\n",
    "\n",
    "import os\n",
    "\n",
    "# For data manipulation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For stats\n",
    "import scipy.stats as spstats\n",
    "\n",
    "# For modeling\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import GetTransform, restyle\n",
    "\n",
    "plt.style.use('./ag_light.mplstyle')"
   ]
  },
  {
   "source": [
    "Also, we can define a quick function that'll help us out later, when it comes to using multi-level columns in pandas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_columns(columns):\n",
    "    \"\"\"Flatten a multilevel column set into '_' separated column names\"\"\"\n",
    "\n",
    "    columns_joined = []\n",
    "    for levels in columns.to_list():\n",
    "        levels = [str(x) for x in levels if x != '']    \n",
    "        columns_joined.append(\"_\".join(levels))\n",
    "\n",
    "    return columns_joined"
   ]
  },
  {
   "source": [
    "## Loading the data\n",
    "\n",
    "We can import the three JSON files  using pandas' built-in read_json function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json files\n",
    "msg = pd.read_json('./json/MESSAGES.json')\n",
    "evnt = pd.read_json('./json/EVENTS.json')\n",
    "usr = pd.read_json('./json/USERS.json')"
   ]
  },
  {
   "source": [
    "Now we need to clean up the dataframes - notably the column names. They'll be less human-friendly, but more code-friendly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack id fields from dict's to strings\n",
    "func = lambda x: x['$oid']\n",
    "msg[['_id','user']] = msg[['_id','user']].applymap(func)\n",
    "evnt[['_id','user']] = evnt[['_id','user']].applymap(func)\n",
    "usr['_id'] = usr['_id'].apply(func)\n",
    "\n",
    "usr.rename(columns={'_id':'user'}, inplace=True)\n",
    "\n",
    "# Fix column names in events dataframe\n",
    "code_title = {\n",
    "    'Added new food diary entry': 'diary',\n",
    "    'Sent message': 'message',\n",
    "    'Read article': 'article',\n",
    "    'Weigh-in': 'weight',\n",
    "    'Saved recipe': 'recipe'\n",
    "}\n",
    "evnt['title'] = evnt['title'].apply(lambda x: code_title.get(x,x))\n",
    "evnt.rename(columns={'weekOnProgramme':'weekNumber'}, inplace=True)"
   ]
  },
  {
   "source": [
    "Next, we'll set up the primary dataframe to house our data. To initialise it, we just make a row for each week, for each user"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                          user churnedAfterSix  weekNumber\n0     5a2e417806d240124a6185a0              NA           0\n1     5a2e417806d240124a6185a0              NA           1\n2     5a2e417806d240124a6185a0              NA           2\n3     5a2e417806d240124a6185a0              NA           3\n4     5a2e417806d240124a6185a0              NA           4\n...                        ...             ...         ...\n3061  5ce6663e7de10312a89e572b           False           2\n3062  5ce6663e7de10312a89e572b           False           3\n3063  5ce6663e7de10312a89e572b           False           4\n3064  5ce6663e7de10312a89e572b           False           5\n3065  5ce6663e7de10312a89e572b           False           6\n\n[3066 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data_weekly = usr['user'].repeat(7).reset_index(drop=True).to_frame()\n",
    "data_weekly = pd.merge(data_weekly, usr[['user','churnedAfterSix']], on='user')\n",
    "data_weekly['weekNumber'] = np.tile(range(7), usr['user'].nunique())\n",
    "\n",
    "print(data_weekly)"
   ]
  },
  {
   "source": [
    "## Data prep (events data)\n",
    "\n",
    "We can start to populate this dataframe by transforming the data from EVENTS.json. Here, for each user we want to know how many of each event they performed, per week. We'll impute missing values with 0, as a missing entry implies they did not do the event that week. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'Number of each event, by week'\n",
    "data_events = (\n",
    "    evnt.groupby(['user','weekNumber','title'])\n",
    "    .count()\n",
    "    .unstack(['title'])\n",
    ")\n",
    "data_events.columns = data_events.columns.droplevel(0)\n",
    "data_events = pd.merge(data_weekly[['user','weekNumber']], data_events, on=['user','weekNumber'], how='left')\n",
    "\n",
    "# Fill missing values with 0\n",
    "tf = data_events.columns.str.contains('arti|diar|mess|reci|weig')\n",
    "data_events.loc[:,tf] = data_events.loc[:,tf].fillna(0)"
   ]
  },
  {
   "source": [
    "We can also generate an additional two features. First is the *total number* of events logged, per user, per week. Though, we don't include 'messages sent' in this, as we will perform separate analyses on the message dataset in a sec.\n",
    "\n",
    "The second is the 'change in no. events', by week. Essentially, this indicates if the user is doing more or fewer events, week on week. We'll add the prefix 'delta_' to these features, as the delta symbol ($\\Delta$) is commonly used to denote 'change of' a given value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       user  weekNumber  article  diary  message  recipe  \\\n",
       "0  5a2e417806d240124a6185a0           0      0.0    1.0      3.0     0.0   \n",
       "1  5a2e417806d240124a6185a0           1      5.0    1.0      0.0     0.0   \n",
       "2  5a2e417806d240124a6185a0           2      0.0    0.0      0.0     0.0   \n",
       "3  5a2e417806d240124a6185a0           3      4.0    0.0      2.0     0.0   \n",
       "4  5a2e417806d240124a6185a0           4      0.0    0.0      0.0     0.0   \n",
       "\n",
       "   weight  numberEvents  delta_numberEvents  \n",
       "0     0.0           1.0                 0.0  \n",
       "1     2.0           8.0                 7.0  \n",
       "2     1.0           1.0                -7.0  \n",
       "3     2.0           6.0                 5.0  \n",
       "4     1.0           1.0                -5.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>weekNumber</th>\n      <th>article</th>\n      <th>diary</th>\n      <th>message</th>\n      <th>recipe</th>\n      <th>weight</th>\n      <th>numberEvents</th>\n      <th>delta_numberEvents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5a2e417806d240124a6185a0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5a2e417806d240124a6185a0</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5a2e417806d240124a6185a0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-7.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5a2e417806d240124a6185a0</td>\n      <td>3</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5a2e417806d240124a6185a0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>-5.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Add 'Change in...' metrics\n",
    "tf = data_events.columns.str.contains('arti|diar|reci|weig')\n",
    "data_events['numberEvents'] = data_events.loc[:,tf].sum(1)\n",
    "data_events['delta_numberEvents'] = data_events.groupby('user')['numberEvents'].diff().fillna(0)\n",
    "\n",
    "data_events.head()"
   ]
  },
  {
   "source": [
    "This can now be appended to our main dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to main dataframe\n",
    "data_weekly = pd.merge(data_weekly, data_events, on=['user','weekNumber'], how='left')\n",
    "del(data_events)"
   ]
  },
  {
   "source": [
    "## Data prep (message data)\n",
    "\n",
    "For the message dataset we want to calculate the average usage for each user, per week, and per messageType (group or private message)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated average metrics, per user, per week\n",
    "msg_weekly = (\n",
    "    msg.groupby(['user','weekNumber','messageType'])\n",
    "    .agg(\n",
    "        messageCount = ('sentiment','count'),\n",
    "        sentiment = ('sentiment', 'mean'),\n",
    "        questionsAsked = ('questionsAsked', 'sum'),\n",
    "        emojisUsed = ('emojisUsed', 'sum'),\n",
    "        mentionedScales = ('mentionedScales', lambda x: int(any(x>0))),\n",
    "        mentionedTracker = ('mentionedTracker', lambda x: int(any(x>0))),\n",
    "    )\n",
    "    .unstack(['messageType'])\n",
    "    .reset_index()\n",
    ")\n",
    "msg_weekly.columns = flatten_columns(msg_weekly.columns)\n",
    "\n",
    "# Expand rows to include weeks where users have no messages logged\n",
    "msg_weekly = pd.merge(\n",
    "    data_weekly[['user','weekNumber','churnedAfterSix']],\n",
    "    msg_weekly, \n",
    "    on=['user','weekNumber'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Impute missing values with 0\n",
    "tf = msg_weekly.columns.str.contains('mess|quest|mention|emoji')\n",
    "msg_weekly.loc[:,tf] = msg_weekly.loc[:,tf].fillna(0)"
   ]
  },
  {
   "source": [
    "We can engineer more 'change in x' features at this point, for sentiment, messageCount, questionAsked, mentionedTracker and mentionedScales."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Engineer additional features (message data)\n",
    "\n",
    "# Calculate change in sentiment & no. messages\n",
    "tf = msg_weekly.columns.str.contains('^senti|^messa|^quest|^emoji|^menti')\n",
    "delta_sentiment = (\n",
    "    msg_weekly.loc[:,tf]\n",
    "    .groupby(msg_weekly['user'])\n",
    "    .diff(axis=0)\n",
    "    .fillna(0)\n",
    "    .add_prefix('delta_')\n",
    ")\n",
    "msg_weekly = pd.concat((msg_weekly,delta_sentiment), axis=1)\n",
    "msg_weekly.drop(columns=['churnedAfterSix'], inplace=True)"
   ]
  },
  {
   "source": [
    "Again, we add append this to our main dataframe, and save a copy of the dataframe (in case we want to peruse it at a later point)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                          user churnedAfterSix  weekNumber  article  diary  \\\n0     5a2e417806d240124a6185a0              NA           0      0.0    1.0   \n1     5a2e417806d240124a6185a0              NA           1      5.0    1.0   \n2     5a2e417806d240124a6185a0              NA           2      0.0    0.0   \n3     5a2e417806d240124a6185a0              NA           3      4.0    0.0   \n4     5a2e417806d240124a6185a0              NA           4      0.0    0.0   \n...                        ...             ...         ...      ...    ...   \n3061  5ce6663e7de10312a89e572b           False           2      6.0    6.0   \n3062  5ce6663e7de10312a89e572b           False           3      6.0    0.0   \n3063  5ce6663e7de10312a89e572b           False           4      0.0    0.0   \n3064  5ce6663e7de10312a89e572b           False           5     10.0    0.0   \n3065  5ce6663e7de10312a89e572b           False           6      0.0    0.0   \n\n      message  recipe  weight  numberEvents  delta_numberEvents  ...  \\\n0         3.0     0.0     0.0           1.0                 0.0  ...   \n1         0.0     0.0     2.0           8.0                 7.0  ...   \n2         0.0     0.0     1.0           1.0                -7.0  ...   \n3         2.0     0.0     2.0           6.0                 5.0  ...   \n4         0.0     0.0     1.0           1.0                -5.0  ...   \n...       ...     ...     ...           ...                 ...  ...   \n3061     17.0     1.0     5.0          18.0                -5.0  ...   \n3062     21.0     0.0     6.0          12.0                -6.0  ...   \n3063      3.0     0.0     1.0           1.0               -11.0  ...   \n3064      1.0     0.0     3.0          13.0                12.0  ...   \n3065      1.0     0.0     0.0           0.0               -13.0  ...   \n\n      delta_sentiment_group  delta_sentiment_private  \\\n0                  0.000000                 0.000000   \n1                  0.000000                 0.000000   \n2                  0.000000                 0.000000   \n3                  0.000000                 0.000000   \n4                  0.000000                 0.000000   \n...                     ...                      ...   \n3061              -0.033751                 0.000000   \n3062               0.118789                 0.071357   \n3063               0.085950                 0.149366   \n3064              -0.114580                 0.000000   \n3065               0.000000                 0.000000   \n\n      delta_questionsAsked_group  delta_questionsAsked_private  \\\n0                            0.0                           0.0   \n1                            0.0                           0.0   \n2                            0.0                           0.0   \n3                            0.0                           0.0   \n4                            0.0                           0.0   \n...                          ...                           ...   \n3061                        -1.0                           0.0   \n3062                         4.0                           0.0   \n3063                        -5.0                           0.0   \n3064                         0.0                           0.0   \n3065                         0.0                           2.0   \n\n      delta_emojisUsed_group  delta_emojisUsed_private  \\\n0                        0.0                       0.0   \n1                        0.0                       0.0   \n2                        0.0                       0.0   \n3                        0.0                       0.0   \n4                        0.0                       0.0   \n...                      ...                       ...   \n3061                     5.0                       0.0   \n3062                    -1.0                       1.0   \n3063                    -7.0                      -1.0   \n3064                    -1.0                       0.0   \n3065                     0.0                       0.0   \n\n      delta_mentionedScales_group  delta_mentionedScales_private  \\\n0                             0.0                            0.0   \n1                             0.0                            0.0   \n2                             0.0                            0.0   \n3                             0.0                            0.0   \n4                             0.0                            0.0   \n...                           ...                            ...   \n3061                          0.0                            0.0   \n3062                          0.0                            0.0   \n3063                          0.0                            0.0   \n3064                          0.0                            0.0   \n3065                          0.0                            0.0   \n\n      delta_mentionedTracker_group  delta_mentionedTracker_private  \n0                              0.0                             0.0  \n1                              0.0                             0.0  \n2                              0.0                             0.0  \n3                              0.0                             0.0  \n4                              0.0                             0.0  \n...                            ...                             ...  \n3061                           0.0                             0.0  \n3062                           0.0                             0.0  \n3063                           0.0                             0.0  \n3064                           0.0                             0.0  \n3065                           0.0                             0.0  \n\n[3066 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Append to main dataframe\n",
    "data_weekly = pd.merge(data_weekly, msg_weekly, how='left', on=['user','weekNumber'])\n",
    "del(msg_weekly)\n",
    "\n",
    "# Save reformatted data\n",
    "# data_weekly.round(3).to_csv(os.path.join('csv','data_user_weekly.csv'))\n",
    "print(data_weekly)"
   ]
  },
  {
   "source": [
    "## Format data for model\n",
    "\n",
    "The last thing we're going to do before running any correlations is unstack the dataframe, so that each column contains a variable for a specific week."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(438, 220)\n"
     ]
    }
   ],
   "source": [
    "# Format data\n",
    "data_model = data_weekly.pivot(index=['user','churnedAfterSix'],columns='weekNumber').reset_index('churnedAfterSix')\n",
    "data_user = usr[['user','age','height','weight']].set_index('user')\n",
    "\n",
    "col_names = list(zip(*[np.repeat('userData', data_user.shape[1]),data_user.columns]))\n",
    "data_user.columns = pd.MultiIndex.from_tuples(col_names)\n",
    "\n",
    "data_model = pd.merge(data_user,data_model, on='user').set_index('churnedAfterSix',append=True)\n",
    "print(data_model.shape)"
   ]
  },
  {
   "source": [
    "Given that we're analysing each metric at each time point, and additional generated features for each metric, we've now got 220 features that can be analysed. In hindsight, this was probably overkill for the task at hand, but offered the greatest breadth of metrics for our model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Correlation analysis (continuous data)\n",
    "\n",
    "With all our continuous variables arranged, we'll now run a pearsons correlation on each of the variables to determine whether they correlate significantly with incidence of churning. This is also commonly referred to as a point bisceral correlation, but for all intents and purposes, it's a pearsons r calculation.\n",
    "\n",
    "We'll print those results with the greatest significance, and save all the stats to a csv file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    week                      features nFalse aveFalse nTrue aveTrue  \\\n",
       "3      0                       article  390.0    4.346  38.0   2.526   \n",
       "154    4         delta_sentiment_group  390.0   -0.003  38.0   0.041   \n",
       "75     2             sentiment_private  232.0    0.153  23.0    0.23   \n",
       "61     2          messageCount_private  390.0    2.128  38.0   3.342   \n",
       "93     6        questionsAsked_private  390.0    0.103  38.0   0.316   \n",
       "110    2         mentionedScales_group  390.0    0.103  38.0     0.0   \n",
       "144    1    delta_messageCount_private  390.0    1.041  38.0   2.184   \n",
       "208    2  delta_mentionedTracker_group  390.0   -0.023  38.0   0.053   \n",
       "188    3      delta_emojisUsed_private  390.0   -0.177  38.0  -0.553   \n",
       "146    3    delta_messageCount_private  390.0   -0.744  38.0  -1.947   \n",
       "\n",
       "    pearsonsR   pval  \n",
       "3      -0.119  0.013  \n",
       "154     0.106  0.028  \n",
       "75      0.136   0.03  \n",
       "61      0.102  0.035  \n",
       "93      0.101  0.036  \n",
       "110      -0.1  0.038  \n",
       "144     0.094  0.053  \n",
       "208     0.093  0.054  \n",
       "188    -0.093  0.056  \n",
       "146    -0.092  0.057  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>week</th>\n      <th>features</th>\n      <th>nFalse</th>\n      <th>aveFalse</th>\n      <th>nTrue</th>\n      <th>aveTrue</th>\n      <th>pearsonsR</th>\n      <th>pval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>article</td>\n      <td>390.0</td>\n      <td>4.346</td>\n      <td>38.0</td>\n      <td>2.526</td>\n      <td>-0.119</td>\n      <td>0.013</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>4</td>\n      <td>delta_sentiment_group</td>\n      <td>390.0</td>\n      <td>-0.003</td>\n      <td>38.0</td>\n      <td>0.041</td>\n      <td>0.106</td>\n      <td>0.028</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>2</td>\n      <td>sentiment_private</td>\n      <td>232.0</td>\n      <td>0.153</td>\n      <td>23.0</td>\n      <td>0.23</td>\n      <td>0.136</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2</td>\n      <td>messageCount_private</td>\n      <td>390.0</td>\n      <td>2.128</td>\n      <td>38.0</td>\n      <td>3.342</td>\n      <td>0.102</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>6</td>\n      <td>questionsAsked_private</td>\n      <td>390.0</td>\n      <td>0.103</td>\n      <td>38.0</td>\n      <td>0.316</td>\n      <td>0.101</td>\n      <td>0.036</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>2</td>\n      <td>mentionedScales_group</td>\n      <td>390.0</td>\n      <td>0.103</td>\n      <td>38.0</td>\n      <td>0.0</td>\n      <td>-0.1</td>\n      <td>0.038</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>1</td>\n      <td>delta_messageCount_private</td>\n      <td>390.0</td>\n      <td>1.041</td>\n      <td>38.0</td>\n      <td>2.184</td>\n      <td>0.094</td>\n      <td>0.053</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>2</td>\n      <td>delta_mentionedTracker_group</td>\n      <td>390.0</td>\n      <td>-0.023</td>\n      <td>38.0</td>\n      <td>0.053</td>\n      <td>0.093</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>3</td>\n      <td>delta_emojisUsed_private</td>\n      <td>390.0</td>\n      <td>-0.177</td>\n      <td>38.0</td>\n      <td>-0.553</td>\n      <td>-0.093</td>\n      <td>0.056</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>3</td>\n      <td>delta_messageCount_private</td>\n      <td>390.0</td>\n      <td>-0.744</td>\n      <td>38.0</td>\n      <td>-1.947</td>\n      <td>-0.092</td>\n      <td>0.057</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Remove the data where churn is not known\n",
    "data_pearson = data_model.query(\"churnedAfterSix != 'NA'\")\n",
    "\n",
    "stats = []\n",
    "# loop through each column in the dataframe\n",
    "for col in data_pearson.columns.to_list():\n",
    "\n",
    "    # x will be binary (0 or 1), y will be continuous\n",
    "    x = data_pearson.index.get_level_values('churnedAfterSix').to_numpy(int)\n",
    "    y = data_pearson[col].to_numpy()\n",
    "    \n",
    "    # filter out values where there are nan's - only relevant for sentiment column\n",
    "    tf = ~np.isnan(y)\n",
    "    x,y = x[tf],y[tf]\n",
    "    \n",
    "    # calculate a couple of additional stats\n",
    "    meanFalse = y[x==0].mean()\n",
    "    meanTrue = y[x==1].mean()\n",
    "    nFalse = len(y[x==0])\n",
    "    nTrue = len(y[x==1])\n",
    "    \n",
    "    # run the correlation\n",
    "    r,p = spstats.pearsonr(x,y)\n",
    "    \n",
    "    # append results to list of stats\n",
    "    res = [col[1], col[0], nFalse, meanFalse, nTrue, meanTrue, r, p]\n",
    "    stats.append(res)\n",
    "        \n",
    "# Create, format and sort stats dataframe     \n",
    "stats = np.asarray(stats)\n",
    "dfStats = pd.DataFrame(data=stats, columns=['week', 'features', 'nFalse', 'aveFalse', 'nTrue', 'aveTrue', 'pearsonsR', 'pval'])\n",
    "dfStats.iloc[:,2:] = dfStats.iloc[:,2:].astype(float).round(3)\n",
    "dfStats = dfStats.sort_values(by=['pval'], ascending=True)\n",
    "\n",
    "# Save summary stats\n",
    "dfStats = dfStats.dropna(axis=0)\n",
    "# dfStats.to_csv(os.path.join('csv','stats_pearsonr_group_weekly.csv'))\n",
    "\n",
    "# Print results\n",
    "dfStats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    week                features nFalse aveFalse nTrue aveTrue pearsonsR  \\\n",
       "3      0                 article  390.0    4.346  38.0   2.526    -0.119   \n",
       "154    4   delta_sentiment_group  390.0   -0.003  38.0   0.041     0.106   \n",
       "75     2       sentiment_private  232.0    0.153  23.0    0.23     0.136   \n",
       "61     2    messageCount_private  390.0    2.128  38.0   3.342     0.102   \n",
       "93     6  questionsAsked_private  390.0    0.103  38.0   0.316     0.101   \n",
       "79     6       sentiment_private  102.0    0.145   8.0   0.053     -0.14   \n",
       "78     5       sentiment_private   81.0    0.133  11.0   0.188     0.114   \n",
       "\n",
       "      pval  \n",
       "3    0.013  \n",
       "154  0.028  \n",
       "75    0.03  \n",
       "61   0.035  \n",
       "93   0.036  \n",
       "79   0.144  \n",
       "78   0.281  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>week</th>\n      <th>features</th>\n      <th>nFalse</th>\n      <th>aveFalse</th>\n      <th>nTrue</th>\n      <th>aveTrue</th>\n      <th>pearsonsR</th>\n      <th>pval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>article</td>\n      <td>390.0</td>\n      <td>4.346</td>\n      <td>38.0</td>\n      <td>2.526</td>\n      <td>-0.119</td>\n      <td>0.013</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>4</td>\n      <td>delta_sentiment_group</td>\n      <td>390.0</td>\n      <td>-0.003</td>\n      <td>38.0</td>\n      <td>0.041</td>\n      <td>0.106</td>\n      <td>0.028</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>2</td>\n      <td>sentiment_private</td>\n      <td>232.0</td>\n      <td>0.153</td>\n      <td>23.0</td>\n      <td>0.23</td>\n      <td>0.136</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>2</td>\n      <td>messageCount_private</td>\n      <td>390.0</td>\n      <td>2.128</td>\n      <td>38.0</td>\n      <td>3.342</td>\n      <td>0.102</td>\n      <td>0.035</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>6</td>\n      <td>questionsAsked_private</td>\n      <td>390.0</td>\n      <td>0.103</td>\n      <td>38.0</td>\n      <td>0.316</td>\n      <td>0.101</td>\n      <td>0.036</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>6</td>\n      <td>sentiment_private</td>\n      <td>102.0</td>\n      <td>0.145</td>\n      <td>8.0</td>\n      <td>0.053</td>\n      <td>-0.14</td>\n      <td>0.144</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>5</td>\n      <td>sentiment_private</td>\n      <td>81.0</td>\n      <td>0.133</td>\n      <td>11.0</td>\n      <td>0.188</td>\n      <td>0.114</td>\n      <td>0.281</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dfStats.query(\"pearsonsR > 0.1 | pearsonsR < -0.1\")\n"
   ]
  },
  {
   "source": [
    "Given that we're looking at over 200 features, we would typically look to set a strict significance threshold (alpha value), above which we would discard features. This would reduce the chance that one of our significant correlations occured simply by chance. Unfortunately, our smallest p value is $p = 0.013$, so we should not outright conclude that any of our correlations are statistically significant (typical alpha value might be 0.01, or 0.001). \n",
    "\n",
    "However, given that we're trying to build a model to predict churn, it is less important that the p values lie below a certain threshold. Ultimately, our model will take in numerous features as input, so it's likely that the interaction between features will be more important than their standalone correlation with churn. \n",
    "\n",
    "Some features crop up more than others in this list, including the message count and number of questions asked in private message. These features may well show covariance, as if you have more questions, you'll likely send more messages. \n",
    "\n",
    "The variable that gives the most significant correlation is the number of articles a user reads prior to starting the programme (week 0).\n",
    "\n",
    "Interestingly, the strongest correlations (pearson's R value, rather than p value), arise from sentiment in private messages. These are less significant, because there are a number of missing values in the dataset (users didn't necessary log a message every week). However they may still be interesting to look at."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Visualising continuous data\n",
    "\n",
    "Now we'll plot each variable as a timeseries, to see how the variables change over the course of the six week period.\n",
    "\n",
    "First we need to set up some parameters for the plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean weekly values\n",
    "data_mean = data_model.groupby('churnedAfterSix').mean()\n",
    "data_mean.round(3).T.to_csv(os.path.join('csv','data_average_weekly.csv'))\n",
    "\n",
    "# Adjust y scale for mentionedScales and mentionedTracker\n",
    "tf = data_mean.columns.get_level_values(0).str.contains('^mention')\n",
    "data_mean.loc[:,tf] = data_mean.loc[:,tf]*100\n",
    "\n",
    "# Set up plot parameters\n",
    "titles = {\n",
    "    'messageCount': \"Average no. messages sent, per user\",\n",
    "    'sentiment': \"Average sentiment, per user\",\n",
    "    'questionsAsked': \"Average no. questions asked, per user\",\n",
    "    'mentionedScales': \"% of users that mentioned 'scales'\",\n",
    "    'mentionedTracker': \"% of users that mentioned 'tracker'\",\n",
    "    'emojisUsed': \"Average no. times a user used an emoji\",\n",
    "    'article': \"no. Articles read, per user\",\n",
    "    'diary': \"no. Food diary entries added, per user\",\n",
    "    'weight': \"no. Weigh-ins, per user\",\n",
    "    'recipe': \"no. Recipes saved, per user\",\n",
    "    'numberEvents': \"no. Events logged, per user\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    False: 'black',\n",
    "    True: (1,0,0)\n",
    "}\n",
    "\n",
    "limits = {\n",
    "    'sentiment': [0, 0.25],\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    True: 'Churned',\n",
    "    False: 'Completed'\n",
    "}"
   ]
  },
  {
   "source": [
    "Then we can generate the plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 6})\n",
    "fig = plt.figure(figsize=(3,2))\n",
    "\n",
    "for col in data_mean.columns.get_level_values(0).unique().to_list():\n",
    "    # Skip column if not in 'titles'\n",
    "    col_parts = col.split('_')\n",
    "    if col_parts[0] not in titles.keys():\n",
    "        continue\n",
    "    \n",
    "    # Format data\n",
    "    data = data_mean.query(\"churnedAfterSix != 'NA'\")[col].T#.unstack('churnedAfterSix')\n",
    "    \n",
    "    # Unpack variables to plot\n",
    "    x = data.index.to_numpy() * [[1],[1]]\n",
    "    y = data.to_numpy().T\n",
    "    label = [labels[c] for c in data.columns]\n",
    "    color = [colors[c] for c in data.columns]\n",
    "    \n",
    "    # Plot the lines\n",
    "    ax = fig.add_axes([0.15,0.2,0.7,0.6])\n",
    "    ax.plot(x[0], y[0], c=color[0], lw=2)\n",
    "    ax.plot(x[1], y[1], c=color[1], lw=2)\n",
    "    \n",
    "    # Tidy up plot\n",
    "    restyle(ax)\n",
    "    ax.set_xticks(np.arange(0,7,2))\n",
    "    ax.set_ylim(ymin=0)\n",
    "    if col_parts[0] in limits.keys():\n",
    "        ax.set_ylim(limits[col_parts[0]])\n",
    "    \n",
    "    # Add labels\n",
    "    trans = GetTransform(ax,system='8pt', anchor='tl')\n",
    "    ax.text(0,2,titles[col_parts[0]], size=8, weight='bold', transform=trans)\n",
    "    if col_parts[-1] in ['private', 'group']:\n",
    "        ax.text(0,1,f'{col_parts[-1].title()} messages', size=6, transform=trans)\n",
    "    \n",
    "    ax.set_xlabel('Week')\n",
    "    \n",
    "    # Add legend\n",
    "    trans = GetTransform(ax, system='8pt', anchor='tr')\n",
    "    ax.text(0,-1, label[False], c=colors[False], size=6, weight='bold', ha='right', transform=trans)\n",
    "    ax.text(0,-2, label[True], c=colors[True], size=6, weight='bold', ha='right', transform=trans)\n",
    "    \n",
    "    # Save plot\n",
    "    filename = os.path.join('.','figs',f'{col}.png')\n",
    "    # fig.savefig(filename, dpi=450)\n",
    "    # fig.savefig(filename.replace('.png','.svg'))\n",
    "    fig.clear()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "source": [
    "Below are an example of how these plots look.\n",
    "\n",
    "![](figs/article.svg)\n",
    "![](figs/messageCount_private.svg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Correlation analysis (discrete data)\n",
    "\n",
    "Now we can assess the correlation between our discrete variables and churn. For example, we can ask whether identifying as male or female significantly changes the likelihood that a user will churn. To do this we'll use the chi-squared test, which takes a contingency as input, and returns the correlation and p values associated with that table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   chi2   pval\n",
       "motivation_other  1.275  0.259\n",
       "challenge_time    0.644  0.422\n",
       "trigger_routine   0.624  0.430\n",
       "trigger_emotions  0.547  0.459\n",
       "gender_M          0.477  0.490"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chi2</th>\n      <th>pval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>motivation_other</th>\n      <td>1.275</td>\n      <td>0.259</td>\n    </tr>\n    <tr>\n      <th>challenge_time</th>\n      <td>0.644</td>\n      <td>0.422</td>\n    </tr>\n    <tr>\n      <th>trigger_routine</th>\n      <td>0.624</td>\n      <td>0.430</td>\n    </tr>\n    <tr>\n      <th>trigger_emotions</th>\n      <td>0.547</td>\n      <td>0.459</td>\n    </tr>\n    <tr>\n      <th>gender_M</th>\n      <td>0.477</td>\n      <td>0.490</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Format data\n",
    "dummy_cols = ['goalsAspiration', 'gender', 'motivation', 'challenge', 'trigger']\n",
    "dummies = pd.get_dummies(usr[dummy_cols], dummy_na=True) # In this instance, we dont want to 'drop_first'\n",
    "data_quiz = dummies.groupby(usr.churnedAfterSix).agg(['sum',lambda x: x.shape[0]])\n",
    "data_quiz = data_quiz.query(\"churnedAfterSix != 'NA'\")\n",
    "\n",
    "chi = []; pval = []; index = []\n",
    "for col in set(data_quiz.columns.get_level_values(0)):\n",
    "    try:\n",
    "        c,p,*_ = spstats.chi2_contingency(data_quiz[col])\n",
    "        # c,p = spstats.fisher_exact(data_quiz[col])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    chi.append(round(c,3))\n",
    "    pval.append(round(p,3))\n",
    "    index.append(col)\n",
    "    \n",
    "chi2 = pd.DataFrame(data=np.asarray([chi,pval]).T, index=index, columns=['chi2','pval'])\n",
    "chi2.sort_values('pval', inplace=True)\n",
    "# chi2.to_csv(os.path.join('csv','stats_chi2_categorical.csv'))\n",
    "\n",
    "chi2.head()\n"
   ]
  },
  {
   "source": [
    "The most significant value returned was $p = 0.259$, suggesting there is little correlation of any of our variables with churn. Still, we can plot the data, as a sanity check, to see if any trends are visually apparent. \n",
    "\n",
    "First we'll define some plot parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat dataframe\n",
    "data_perc = data_quiz.groupby(level=0, axis=1).apply(lambda x: x.iloc[:,0] / x.iloc[:,1]).T\n",
    "index = np.array([string.split('_') for string in data_perc.index])\n",
    "\n",
    "colors = {\n",
    "    False: 'black',\n",
    "    True: (1,0,0)\n",
    "}\n",
    "\n",
    "legend = {\n",
    "    True: 'Churned',\n",
    "    False: 'Completed'\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 6,\n",
    "    'axes.grid.axis': 'x',\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'axes.ymargin': .1,\n",
    "})"
   ]
  },
  {
   "source": [
    "Then we'll generate the plots."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the groups (column fields)\n",
    "groups = set(index[:,0])\n",
    "fig = plt.figure(figsize=(2,3))\n",
    "\n",
    "for grp in groups:\n",
    "    # Select the appropriate columns & y labels\n",
    "    tf = data_perc.index.str.contains(f'^{grp}')\n",
    "    labels = data_perc[tf].index.to_numpy()\n",
    "    labels = [lab.split('_')[-1] for lab in labels]\n",
    "    \n",
    "    # Pull the x and y data\n",
    "    xl,xr = data_perc.loc[tf].to_numpy().T * [[-1],[1]]\n",
    "    y = np.arange(len(xl))\n",
    "    \n",
    "    # Draw the boxes\n",
    "    ax = fig.add_axes([0.2,0.2,0.5,0.6])\n",
    "    ax.barh(y, xl, 0.5, fc=colors[False])\n",
    "    ax.barh(y, xr, 0.5, fc=colors[True])\n",
    "    \n",
    "    # Clean up axes\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlim([-1,1])\n",
    "    ax.set_xticklabels([100,50,0,50,100])\n",
    "    ax.set_xlabel(\"% of users\")\n",
    "    # Draw y labels\n",
    "    for pos,lab in zip(y,labels):\n",
    "        ax.text(1.05, pos, lab, va='center', transform=ax.transData)\n",
    "    \n",
    "    # Add title\n",
    "    trans = GetTransform(ax,system='10pt', anchor='tl')\n",
    "    ax.text(0,3, f\"% of responses to\", size=8, weight='bold', transform=trans)\n",
    "    ax.text(0,2, f\"'{grp}' question\", size=8, weight='bold', transform=trans)\n",
    "    \n",
    "    # Draw legend\n",
    "    trans = GetTransform(ax, system=('data','6pt'), anchor='bl')\n",
    "    ax.text(-1, 0, legend[False], c=colors[False], size=6, weight='bold', transform=trans)\n",
    "    ax.text(1, 0, legend[True], c=colors[True], size=6, weight='bold', ha='right', transform=trans)\n",
    "    \n",
    "    # Save figure\n",
    "    filename = os.path.join('.','figs',f'{grp}.png')\n",
    "    # fig.savefig(filename, dpi=450)\n",
    "    # fig.savefig(filename.replace('.png','.svg'), dpi=450)\n",
    "    fig.clear()\n",
    "    \n",
    "plt.close()"
   ]
  },
  {
   "source": [
    "The plots look like those shown below.\n",
    "\n",
    "They show the percentage of users that chose each response to a given sign-up quiz question, split by users that churned (red) and those that completed (black) the programme. While there are some discernible differences visually ('motivation' response to 'challenge' question, or 'social' response to the 'trigger' question), the differences are not so great as to ignore the statistical results from the chi-squared test.\n",
    "\n",
    "![](figs/motivation.svg)\n",
    "![](figs/trigger.svg)\n",
    "![](figs/challenge.svg)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modeling churn\n",
    "\n",
    "Now we're going to try and build a model that can predict whether users will churn or not. To do this, we choose a handful of features that correlate well with churn, then use these to train the model. For this this task I've chosen to use a Random Forest Classification model, predominantly beacuse I haven't used one before and I wanted to learn more about them.\n",
    "\n",
    "First we need to impute the missing values for our sentiment data (the model won't accept nan's). I've decided to impute a user's missing values with the same user's mean, as it seems like most conservative approach to handling these values. Impute with 0's is another option, but this would have skewed the user's mean sentiment.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute user with incomplete data\n",
    "fill_mean = lambda x: x.fillna(x.mean())\n",
    "cols = ['sentiment_private', 'sentiment_group']\n",
    "pivot = (\n",
    "    data_model.loc[:,cols]\n",
    "    .stack(level=1, dropna=False)\n",
    "    .groupby(level=0)\n",
    "    .transform(fill_mean)\n",
    ")\n",
    "\n",
    "# Impute users with no data at all\n",
    "data_model.loc[:,cols] = (\n",
    "    pivot.transform(fill_mean)\n",
    "    .unstack(level=-1)\n",
    ")\n",
    "data_model.reset_index(inplace=True)\n",
    "data_model.set_index(['user','churnedAfterSix'], inplace=True)"
   ]
  },
  {
   "source": [
    "Next we'll select the features with which to train the model and set up the training, test and validation datasets. I've opted to start with a simpler model, so just picked the 8 features with the greatest correlation score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many features we want to include in the model\n",
    "nfeatures = 8\n",
    "\n",
    "# Add absolute correlation, to help sorting\n",
    "dfStats['pearsonsRAbs'] = dfStats['pearsonsR'].abs()\n",
    "\n",
    "# Get the keys (names) for the features\n",
    "keys = (\n",
    "    dfStats.sort_values('pearsonsRAbs', ascending=False)\n",
    "    .loc[:,['week', 'features']][:nfeatures]\n",
    "    .to_numpy()[:,::-1]\n",
    ")\n",
    "keys[:,1] = keys[:,1].astype(int)\n",
    "\n",
    "# Set up the label array (churned or not)\n",
    "churned = data_model.index.get_level_values('churnedAfterSix')\n",
    "tf = churned != 'NA'\n",
    "y = np.array([int(v) for v in churned[tf]])\n",
    "\n",
    "# Normalise the data\n",
    "X_raw = data_model[keys].to_numpy()\n",
    "X = (X_raw - X_raw.mean(0)) / X_raw.std(0)\n",
    "\n",
    "# Set up the train, test and validate sections of the data\n",
    "X_valid = X[~tf,:]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X[tf,:], y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "source": [
    "Finally, we can define, train and test the model. Again, I've opted for a simpler model as a starting point, defining the model only use 100 trees and a maximum branch depth of 5. This leaves room for increasing complexity while tinkering/optimising the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[95  5]\n [ 5  2]]\n\n    Model results:\n    \t accuracy  = 0.907\n    \t recall    = 0.286\n    \t precision = 0.286\n\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=0,\n",
    "    max_depth=5,\n",
    "    class_weight={0:1, 1:8}\n",
    ")\n",
    "\n",
    "# Train the model with the training dataset\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Test the accuracy on the training dataset\n",
    "y_pred = clf.predict(X_train)\n",
    "# skmetrics.confusion_matrix(y_train,y_pred)s\n",
    "\n",
    "# Test the accuracy on the test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(skm.confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calculate model metrics\n",
    "accuracy = skm.accuracy_score(y_test, y_pred)\n",
    "precision = skm.precision_score(y_test, y_pred)\n",
    "recall = skm.recall_score(y_test, y_pred)\n",
    "\n",
    "string = f\"\"\"\n",
    "    Model results:\n",
    "    \\t accuracy  = {accuracy:.3f}\n",
    "    \\t recall    = {recall:.3f}\n",
    "    \\t precision = {precision:.3f}\n",
    "\"\"\"\n",
    "print(string)"
   ]
  },
  {
   "source": [
    "The accuracy of this model is defined as the number of users it correctly classifies as either churner or completer. While the model reveals 90%, this is misleading, as really we're interested in the number of churners, specifically, that we're accurately identifying. For this we look at the model 'precision', which paints a sorrier picture, with only 29% precision (2 out of 7 churners correctly identified).\n",
    "\n",
    "Similarly, we incorrectly identified 5 individuals as churners, when they did not actually churn. This is measured as our 'recall' rate, at 29%.\n",
    "\n",
    "Despite the fact the the precision and recall of this model are far from optimal, this is a decent starting point for further optimisation of the model. For one, we could choose to include more features into the model, as we've been fairly conservative in selecting 8. We could also increase the depth of the trees, which is likely to improve the recall, but at the cost of precision, as the model may start to overfit to the training data. Additionally, we could adjust the weighting of the class (churn vs not churn) to encourage the model to identify more churners, at the risk of identifying more false positives.\n",
    "\n",
    "Last but not least, we can make our predictions for the unknown data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                         user  willChurn\n0    5a2e417806d240124a6185a0      False\n44   5ca8f158540a036b663a34df       True\n55   5cba40489203684bd4c4e04f      False\n58   5cbc31e194b8d6115731aed9      False\n72   5cc929eba4cedc162e6df3c2      False\n134  5cd520832925a912cbdd6061      False\n178  5cd69b69fb622f12c5a74008      False\n230  5cd89558f2f52212d094bd8e       True\n240  5cd976930b932012b968558e      False\n364  5ce050278ff304131108e204      False\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_valid)\n",
    "predictions = usr.query(\"churnedAfterSix == 'NA'\")[['user']]\n",
    "predictions['willChurn'] = y_pred.astype(bool)\n",
    "print(predictions)"
   ]
  }
 ]
}